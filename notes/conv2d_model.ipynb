{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f03dcb9-2e7f-47dd-b39b-b7739a2665d9",
   "metadata": {},
   "source": [
    "# MNIST PyTorch `conv2d` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb3-3848-44cf-b292-275032a56fc6",
   "metadata": {},
   "source": [
    "In this document we\n",
    "\n",
    "- read the training data and plot it, checking labels\n",
    "- reproducibly split the training data\n",
    "- make simplest model and train it for one epoch on CPU and GPU\n",
    "- test logging with [MLflow](https://mlflow.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa40351-e043-472a-8186-e4dad130b6eb",
   "metadata": {},
   "source": [
    "*Table of Contents*\n",
    "\n",
    "1. [Packages](#packages)\n",
    "2. [Config](#config)\n",
    "3. [Data split](#data-split)\n",
    "4. [`DataPipe`s and `DataLoader2`](#dataloader2)\n",
    "5. [As-Simple-As-Possible model](#asap-model)\n",
    "6. [Training](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467f014-6dbe-4268-b3c6-8b806776b075",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"packages\"></a>\n",
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80983f-d1b9-4cdb-a298-abe94964fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.models\n",
    "import mlflow.pytorch\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchdata.datapipes.map import SequenceWrapper\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from omegaconf import OmegaConf as oc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import mlflow\n",
    "\n",
    "import kaggland.digrec.data.data as digrec_data\n",
    "import kaggland.digrec.data.transforms as digrec_transforms\n",
    "\n",
    "import kaggland.utils.preprocessing.aug.image2d as aug_image2d\n",
    "import kaggland.utils.data.dataloader2 as utils_dataloader2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c7670-1a71-4181-83ef-7991b090c1ee",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"config\"></a>\n",
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e071ba-78e1-48d2-ab87-f77f4f1bcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_version = subprocess.run(\n",
    "    [\"python\", \"--version\"],\n",
    "    capture_output=True).stdout.decode(\"utf-8\").replace(\"Python\", \"\").strip()\n",
    "\n",
    "base_config = {\n",
    "    \"info\" : {\n",
    "        \"python_version\" : python_version,\n",
    "        \"numpy_version\" : str(np.__version__),\n",
    "        \"torch_version\" : str(torch.__version__),\n",
    "    },\n",
    "    \"use_cuda\": True,\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    \"data\": {\n",
    "        \"path_to_data\" : pathlib.Path.home() / \"data\" / \"kaggle\" / \"digit-recognizer\" / \"train.csv\",\n",
    "        \"images_datatype\": \"f4\",\n",
    "        \"num_splits\": 10,\n",
    "        \"num_val_splits\": 3,\n",
    "        \"training\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"num_workers\": 2,\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"num_workers\": 2,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e25429-92f8-41e3-8486-242570c1e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model\": {\n",
    "        \"blocks\": [\n",
    "            {\"name\": \"resnet1\",\n",
    "             \"params\": {\n",
    "                 \"in_channels\": 1,\n",
    "                 \"out_channels\": 64,\n",
    "                 \"kernel_size\": (5, 5),\n",
    "                 \"padding\": 2,\n",
    "                 \"conv_weight_init_fn\": \"kaiming_normal_\"},\n",
    "              },\n",
    "            {\"name\": \"resnet2\",\n",
    "             \"params\": {\n",
    "                 \"in_channels\": 64,\n",
    "                 \"out_channels\": 32,\n",
    "                 \"kernel_size\": (3, 3),\n",
    "                 \"padding\": 1,\n",
    "                 \"conv_weight_init_fn\": \"kaiming_normal_\"},\n",
    "            },\n",
    "            {\"name\": \"resnet3\",\n",
    "             \"params\": {\n",
    "                 \"in_channels\": 32,\n",
    "                 \"out_channels\": 16,\n",
    "                 \"kernel_size\": (3, 3),\n",
    "                 \"padding\": 1,\n",
    "                 \"conv_weight_init_fn\": \"kaiming_normal_\"},\n",
    "            },\n",
    "            {\"name\": \"resnet4\",\n",
    "             \"params\": {\n",
    "                 \"in_channels\": 16,\n",
    "                 \"out_channels\": \"${model.num_out_channels}\",\n",
    "                 \"kernel_size\": (3, 3),\n",
    "                 \"padding\": 1,\n",
    "                 \"conv_weight_init_fn\": \"kaiming_normal_\"},\n",
    "             },\n",
    "        ],\n",
    "        \"num_out_channels\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = oc.merge(\n",
    "    oc.create(base_config),\n",
    "    oc.create(data_config),\n",
    "    oc.create(model_config),\n",
    ")\n",
    "oc.resolve(config) \n",
    "\n",
    "config.data, config.model.blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25314de4-f6e0-4fe3-8211-f8e96c375fc6",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"data-split\"></a>\n",
    "## 3. Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ff09-36f4-4b12-80b2-bce3015a17c3",
   "metadata": {},
   "source": [
    "### 3.1 Make split and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd7ec5-62d3-4fec-9e56-cbf0ff0e7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = digrec_data.make(config.data.path_to_data, config.data.images_datatype)\n",
    "train_images = train_data[\"images\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "\n",
    "print(\"unique train_labels:\", np.unique(train_labels))\n",
    "\n",
    "for _, val_img_lab in val_data.items():\n",
    "    val_labels = val_img_lab[\"labels\"]\n",
    "    print(\"unique val_labels:\", np.unique(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f1f47-b0a5-4f01-93b2-f5e2d622e75b",
   "metadata": {},
   "source": [
    "Are the indices in the splits all the indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca129a-7f37-44b6-a9ab-92e7926b7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = train_data[\"indices\"]\n",
    "val_splits = tuple(value[\"indices\"] for value in val_data.values())\n",
    "\n",
    "indices_splits = np.concatenate((train_indices, *val_splits))\n",
    "(np.unique(indices_splits) == np.arange(len(indices_splits))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910874ab-7a62-442a-b4a6-aef13ec9d61a",
   "metadata": {},
   "source": [
    "## 3.2 Util function: compute `train_images` mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1838a-bd65-461b-b9ff-6c3ee17898ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggland.utils.image2d as utils_image2d\n",
    "\n",
    "train_chan_mean, train_chan_std = utils_image2d.compute_channels_mean_std(train_images)\n",
    "print(train_chan_mean, train_chan_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35418a48-c171-41aa-9af6-6e737b0fd8df",
   "metadata": {},
   "source": [
    "### 3.3 Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6aed3-bc9d-4c0c-abcd-94211bff70e8",
   "metadata": {},
   "source": [
    "#### Sanity check plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79faa5-0a70-462d-9fed-43fadbf06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 12\n",
    "# val_images = val_data[0][\"images\"]\n",
    "# val_labels = val_data[0][\"labels\"]\n",
    "# fig, ax = plt.subplots(1, num, figsize=(num*2, 2))\n",
    "# for i in range(num):\n",
    "#     img = val_images[i]\n",
    "#     lab = val_labels[i]\n",
    "#     ax[i].imshow(img, cmap=\"gray\")\n",
    "#     ax[i].set(title=f\"{lab}\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886d20b-1601-418c-b870-ea69f53dbbfe",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"dataloader2\"></a>\n",
    "## 4. `DataPipe`s and `DataLoader2` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b975af-690a-468f-a1bd-2537859d9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91af169a-0b03-459c-a3c1-1158fdf19a7b",
   "metadata": {},
   "source": [
    "### 4.2 Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86283c51-cc86-4bf2-9dd2-c7f79e66024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_MNIST_mini_batch(many_images_labels):\n",
    "    batched_samples = torch.zeros(size=(len(many_images_labels), *many_images_labels[0][0].shape),\n",
    "                                  dtype=torch.float32)\n",
    "    batched_labels = torch.zeros(size=(len(many_images_labels),),\n",
    "                                 dtype=torch.int64)\n",
    "    \n",
    "    for idx, (image, label) in enumerate(many_images_labels):\n",
    "        batched_samples[idx, 0] = torch.from_numpy(image)\n",
    "        batched_labels[idx] = int(label)\n",
    "    \n",
    "    return batched_samples.requires_grad_(True), batched_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dab1a8-a566-4b77-a22c-96ccaef94e0f",
   "metadata": {},
   "source": [
    "### 4.3 Training/validation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984b608-f0ff-4d91-8483-715f2b6b0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MNIST_training_datapipe(train_data, data_config, random_state: int=100):\n",
    "    \"\"\"Make a datapipe for training data.\"\"\"\n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    train_images = train_data[\"images\"]\n",
    "    train_labels = train_data[\"labels\"]\n",
    "    aug_image2d.random_batch_translation(\n",
    "        images=train_images,\n",
    "        labels=train_labels,\n",
    "        max_translation_offset=3,\n",
    "        prob_thresh=0.3,\n",
    "        random_seed=random_state,\n",
    "    )\n",
    "\n",
    "    train_chan_mean, train_chan_std = utils_image2d.compute_channels_mean_std(train_images)\n",
    "    \n",
    "    training_datapipe = (\n",
    "        SequenceWrapper(train_images)\n",
    "        .zip(SequenceWrapper(train_labels))\n",
    "        .shuffle()\n",
    "        .set_seed(0)\n",
    "        .sharding_filter()\n",
    "        .map(partial(digrec_transforms.train_fn,\n",
    "                     chan_mean=train_chan_mean,\n",
    "                     chan_std=train_chan_std))\n",
    "        .batch(data_config.training.batch_size)\n",
    "        .collate(collate_MNIST_mini_batch)\n",
    "        )\n",
    "    \n",
    "    return training_datapipe\n",
    "\n",
    "\n",
    "def make_MNIST_validation_datapipe(train_data, val_data, data_config, idx_val_split: int=0, random_state: int=100):\n",
    "    \"\"\"Make DataPipe for validation data, using the validation split at index `idx_val_split`.\"\"\"\n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    train_images = train_data[\"images\"]\n",
    "    train_labels = train_data[\"labels\"]\n",
    "    val_images = val_data[idx_val_split][\"images\"]\n",
    "    val_labels = val_data[idx_val_split][\"labels\"]\n",
    "    \n",
    "    train_chan_mean, train_chan_std = utils_image2d.compute_channels_mean_std(train_images)\n",
    "    \n",
    "    validation_datapipe = (\n",
    "        SequenceWrapper(val_images)\n",
    "        .zip(SequenceWrapper(val_labels))\n",
    "        .shuffle()\n",
    "        .set_seed(0)\n",
    "        .sharding_filter()\n",
    "        .map(partial(digrec_transforms.train_fn,\n",
    "                     chan_mean=train_chan_mean,\n",
    "                     chan_std=train_chan_std))\n",
    "        .batch(data_config.validation.batch_size)\n",
    "        .collate(collate_MNIST_mini_batch)\n",
    "    )\n",
    "\n",
    "    return validation_datapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e41e03-613c-462a-8eb5-9b6f41474751",
   "metadata": {},
   "source": [
    "### 4.6 DataLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8715821-11e4-4069-80bc-a83c0c3df4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85789eb-5331-49de-9f68-178f28eb16e0",
   "metadata": {},
   "source": [
    "### 4.7 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d168b7-fd68-47f1-acfb-d79bb1ce86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = digrec_data.make(config.data.path_to_data)\n",
    "train_dp = make_MNIST_training_datapipe(train_data, config.data, random_state=100)\n",
    "val0_dp = make_MNIST_validation_datapipe(train_data, val_data=val_data, idx_val_split=0, data_config=config.data, random_state=100)\n",
    "train_loader = utils_dataloader2.make(train_dp, num_workers=config.data.training.num_workers)\n",
    "val0_loader = utils_dataloader2.make(val0_dp, num_workers=config.data.validation.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454aaf5-55fb-47aa-aa23-00fb8b4f1910",
   "metadata": {},
   "source": [
    "#### DAG training DataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd545de-d4bd-4852-bb0f-422fa99a2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchdata.datapipes.utils as dputils\n",
    "# dp_graph = dputils.to_graph(train_dp)\n",
    "# dp_graph.view();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b33912-3d80-404b-ad4c-2e24e050c28a",
   "metadata": {},
   "source": [
    "#### Sample from training DataLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782226-19e5-48de-ab26-0907c4834580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = digrec_data.make(config.data.path_to_data, config.data.images_datatype)\n",
    "train_dp = make_MNIST_training_datapipe(train_data, config.data, random_state=100)\n",
    "val0_dp = make_MNIST_validation_datapipe(train_data, val_data=val_data, idx_val_split=0, data_config=config.data, random_state=100)\n",
    "train_loader = utils_dataloader2.make(train_dp, num_workers=config.data.training.num_workers)\n",
    "val0_loader = utils_dataloader2.make(val0_dp, num_workers=config.data.validation.num_workers)\n",
    "\n",
    "dp_train_loader_iter = iter(train_loader)\n",
    "\n",
    "for _ in range(2):\n",
    "    batch = next(dp_train_loader_iter)\n",
    "    images = batch[0]\n",
    "    labels = batch[1]\n",
    "    print(f\"type(batch): {type(batch)}, l={len(batch)}\")\n",
    "    print(f\"images: (dtype={images.dtype}, shape={images.shape}, mean={images.mean():.3f})\")\n",
    "    print(f\"labels: (dtype={labels.dtype}, shape={labels.shape})\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412fa23-5541-40c4-bdf9-3f2018ccf531",
   "metadata": {},
   "source": [
    "#### Sanity check plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9b3e4-0409-4f3e-9966-ce268ead9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "\n",
    "fig, ax = plt.subplots(1, num, figsize=(num*4, 4))\n",
    "for i, batch in enumerate(train_loader):\n",
    "    image = batch[0][0][0].detach().numpy()\n",
    "    label = batch[1][0]\n",
    "    if i < num:\n",
    "        ax[i].imshow(image, cmap=\"gray\")\n",
    "        ax[i].set(title=f\"{label}, {image.sum():.3f}, {image.dtype}, {image.mean():.3f}\")\n",
    "    else:\n",
    "        break\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdff80-6945-43fe-b55c-f752e93f8fc9",
   "metadata": {},
   "source": [
    "#### Sanity check: `DataLoader2` returns same amounts of digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7b712-06f3-44b9-8853-ebd6649f516b",
   "metadata": {},
   "source": [
    "First, get all `DataPipe` labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a8d02-1d68-47ec-ac3f-cddd0c87215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_labels = torch.zeros(size=(len(train_labels),), dtype=torch.uint8)\n",
    "train_loader = utils_dataloader2.make(train_dp, num_workers=config.data.training.num_workers)\n",
    "\n",
    "idx = 0\n",
    "for batch in train_loader:\n",
    "    for j, el in enumerate(batch[1]):\n",
    "        dp_labels[idx] = el\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d4297-ba8d-4a0a-81a2-866036f0aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1, cnt1 = np.unique(dp_labels, return_counts=True)\n",
    "# u2, cnt2 = np.unique(labels, return_counts=True)\n",
    "u3, cnt3 = np.unique(train_labels, return_counts=True)\n",
    "\n",
    "print(f\"DataPipe  -> u1: {u1}, counts: {cnt1}\")\n",
    "print(f\"Input lab -> u3: {u3}, counts: {cnt3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1113601-6b6c-43d8-be3a-2bd844bb50da",
   "metadata": {},
   "source": [
    "#### Bench: for one epoch `DataPipe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006e45e-1b2a-493d-914a-35ab1c33defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils_dataloader2.make(train_dp, num_workers=config.data.training.num_workers)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "total = 0\n",
    "for batch in train_loader:\n",
    "    images = batch[0][0]\n",
    "    total += images[0][14, 10:12].sum()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time epoch: {end_time - start_time:.3f} seconds\")\n",
    "print(f\"Total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809d114-4b03-4a58-bcd9-395f4ec456b7",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"asap-model\"></a>\n",
    "## 5. As-Simple-As-Possible model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b4ec1-cd90-4648-85f9-d13bec55d68e",
   "metadata": {},
   "source": [
    "### 5.1 Configurable ResNet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e02f2-12fb-4cac-8252-8ac7cfb9e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, resnet_block_config):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=resnet_block_config.in_channels,\n",
    "            out_channels=resnet_block_config.out_channels,\n",
    "            kernel_size=resnet_block_config.kernel_size,\n",
    "            padding=resnet_block_config.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.conv_1x1 = nn.Conv2d(\n",
    "            in_channels=resnet_block_config.in_channels,\n",
    "            out_channels=resnet_block_config.out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(\n",
    "            num_features=resnet_block_config.out_channels\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        conv_weight_init_fn = getattr(torch.nn.init, resnet_block_config.conv_weight_init_fn)\n",
    "        conv_weight_init_fn(self.conv.weight, nonlinearity=\"relu\")\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out_1x1 = self.conv_1x1(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.relu(out)\n",
    "        return out + out_1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8307b49-79d0-407b-89fc-324c43969580",
   "metadata": {},
   "source": [
    "### 5.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c1b8b-8e3d-4b3c-9225-8df6179a969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dModelV1(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Conv2dModelV1, self).__init__()\n",
    "        self.blocks = nn.ModuleDict([\n",
    "            (block_config[\"name\"], ResNetBlock(block_config[\"params\"]))\n",
    "            for block_config in model_config.blocks\n",
    "        ])\n",
    "        self.num_out_channels = model_config.num_out_channels\n",
    "        self.head = nn.ModuleDict([\n",
    "            (\"linear_head\", nn.Linear(28 * 28 * self.num_out_channels, self.num_out_channels)),\n",
    "            (\"batch_norm_head\", nn.BatchNorm1d(num_features=10)),\n",
    "            (\"relu_head\", nn.ReLU()),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks.values():\n",
    "            x = block(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        for block in self.head.values():\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "model = Conv2dModelV1(config.model)\n",
    "print(\"num trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67083a7-bcae-48a1-8fda-7d8ea25d31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2809d-991f-441f-b9dd-6092199fa904",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"training\"></a>\n",
    "## 6. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a6c93-2374-4678-88e8-b317466340fb",
   "metadata": {},
   "source": [
    "### 6.1 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508de35d-a37b-4f1c-88db-f3a87cbd2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(config, n_epochs, model, learning_rate: float=5e-4):\n",
    "    \"\"\"Training loop for MNIST classification task.\"\"\"\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # set MLflow tracking server URI\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5051\")\n",
    "    mlflow.autolog(disable=True)\n",
    "\n",
    "    # start MLflow run\n",
    "    with mlflow.start_run() as run:\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        mlflow.log_param(key=\"n_epochs\", value=n_epochs)\n",
    "        mlflow.log_param(key=\"learning_rate\", value=learning_rate)\n",
    "\n",
    "        # log config\n",
    "        yaml_str = oc.to_yaml(config)\n",
    "        with open(\"config.yaml\", \"w\") as config_file:\n",
    "            config_file.write(yaml_str)\n",
    "        mlflow.log_artifact(\"config.yaml\")\n",
    "        \n",
    "        if config.use_cuda:\n",
    "            model.cuda()\n",
    "        \n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            print(\"Training: \", end=\"\")\n",
    "            model.train(True)\n",
    "            train_data, val_data = digrec_data.make(config.data.path_to_data, config.data.images_datatype)\n",
    "\n",
    "            train_dp = make_MNIST_training_datapipe(\n",
    "                train_data.copy(), \n",
    "                config.data, \n",
    "                random_state=epoch\n",
    "            )\n",
    "            train_loader = utils_dataloader2.make(train_dp, num_workers=config.data.training.num_workers)\n",
    "            epoch_start_time = time.time()\n",
    "            for idx_batch, batch in enumerate(train_loader):\n",
    "                samples, targets = batch[0], batch[1]\n",
    "                if config.use_cuda:\n",
    "                    samples = samples.to(device=\"cuda\")\n",
    "                    targets = targets.to(device=\"cuda\")\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                output_model = model(samples)\n",
    "                loss = loss_fn(output_model, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                if idx_batch % 200 == 0:\n",
    "                  print(\".\", end=\"\", flush=True)\n",
    "            print()\n",
    "            \n",
    "            print(\"Validation: \", end=\"\")\n",
    "            model.train(False)\n",
    "            val0_dp = make_MNIST_validation_datapipe(\n",
    "                train_data.copy(), \n",
    "                val_data=val_data.copy(), \n",
    "                idx_val_split=0, \n",
    "                data_config=config.data, \n",
    "                random_state=epoch\n",
    "            )\n",
    "            val0_loader = utils_dataloader2.make(val0_dp, num_workers=config.data.validation.num_workers)\n",
    "            with torch.inference_mode():\n",
    "                val_results_current_epoch = {\"correct\": 0, \"total\": 0}\n",
    "                for idx_batch, batch in enumerate(val0_loader):\n",
    "                    samples, targets = batch[0], batch[1]\n",
    "                    if config.use_cuda:\n",
    "                        samples = samples.cuda()\n",
    "                    \n",
    "                    output_model = model(samples)\n",
    "                    model_predictions = torch.argmax(output_model, dim=1)\n",
    "                    \n",
    "                    signature = mlflow.models.infer_signature(samples.to(device=\"cpu\").numpy(), model_predictions.to(device=\"cpu\").numpy())\n",
    "                    \n",
    "                    if config.use_cuda:\n",
    "                        model_predictions = model_predictions.to(device=\"cpu\").numpy()\n",
    "                    expected_predictions = targets.numpy()\n",
    "                    \n",
    "                    num_correct_predictions = (model_predictions == expected_predictions).sum()\n",
    "                    val_results_current_epoch[\"correct\"] += num_correct_predictions\n",
    "                    val_results_current_epoch[\"total\"] += len(expected_predictions)\n",
    "                \n",
    "                    if idx_batch % 200 == 0:\n",
    "                        print(\".\", end=\"\", flush=True)\n",
    "            print()\n",
    "            # ---\n",
    "            accuracy = val_results_current_epoch['correct'] / val_results_current_epoch['total']\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            print(f\" --> Epoch {epoch}\")\n",
    "            print(f\"Final training loss: {loss:.3f}\")\n",
    "            print(f\"Validation correct predictions: {100*(val_results_current_epoch['correct'] / val_results_current_epoch['total']):.2f}%\")\n",
    "            print(f\"Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "            if epoch == n_epochs - 1:\n",
    "                mlflow.pytorch.log_model(pytorch_model=model, artifact_path=\"model\", signature=signature)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fd17c-5d05-413d-8975-ccbd493c227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "    config=config,\n",
    "    n_epochs=2,\n",
    "    model=model,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3512980-9894-4853-ba31-63176f92339f",
   "metadata": {},
   "source": [
    "## 7. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccbbf7-a9a5-46c8-8e5f-3fc9816cc7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "model_filename = \"/home/gabri/tmp/pytorch/models/digrec/\" + \"weights-\" + f\"{now.year:05}\" + \"-\" + f\"{now.month:02}\" + \"-\" + f\"{now.day:02}\" + \".pth\"\n",
    "torch.save(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd223e-b31b-4ca2-982d-8eaccddc36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load(model_filename)\n",
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625362a-b266-4915-8f4b-4e04032605d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = make_data(config.data.path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a064e4-355e-4029-a6c7-a84f1167dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val1_dp = make_MNIST_validation_datapipe(\n",
    "    train_data.copy(), \n",
    "    val_data=val_data.copy(), \n",
    "    idx_val_split=1, \n",
    "    data_config=config.data, \n",
    "    random_state=0\n",
    ")\n",
    "val1_loader = make_dataloader2(val1_dp, num_workers=config.data.validation.num_workers)\n",
    "with torch.inference_mode():\n",
    "    val_results_current_epoch = {\"correct\": 0, \"total\": 0}\n",
    "    for idx_batch, batch in enumerate(val1_loader):\n",
    "        if idx_batch > 3:\n",
    "            break\n",
    "            \n",
    "        samples, targets = batch[0], batch[1]\n",
    "        samples = samples.cuda()\n",
    "        \n",
    "        output_model = model(samples)\n",
    "        model_predictions = torch.argmax(output_model, dim=1)\n",
    "        print(f\"{model_predictions=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d9a52-c466-4598-bf8b-c9f8fa7edfda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
